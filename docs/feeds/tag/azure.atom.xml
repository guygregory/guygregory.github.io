<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Pedantic Journal - azure</title><link href="https://pedanticjournal.com/" rel="alternate"/><link href="https://pedanticjournal.com/feeds/tag/azure.atom.xml" rel="self"/><id>https://pedanticjournal.com/</id><updated>2025-09-09T00:00:00+01:00</updated><subtitle>Thoughts on AI and other subjects.</subtitle><entry><title>Using LLM with Azure AI Foundry</title><link href="https://pedanticjournal.com/llm/" rel="alternate"/><published>2025-09-09T00:00:00+01:00</published><updated>2025-09-09T00:00:00+01:00</updated><author><name>Guy Gregory</name></author><id>tag:pedanticjournal.com,2025-09-09:/llm/</id><summary type="html">&lt;p&gt;How to configure Simon Willison's popular LLM CLI tool with Azure AI Foundry models&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Getting Started with LLM&lt;/h3&gt;
&lt;p&gt;I'm a big fan of &lt;a href="https://simonwillison.net/"&gt;Simon Willison's&lt;/a&gt; work, and recently, I had a need to do some simple inference using gpt-5 on Azure AI Foundry. In the past I've used GitHub Models for this type of work, and that works pretty well up to a certain point:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;gh&lt;span class="w"&gt; &lt;/span&gt;models&lt;span class="w"&gt; &lt;/span&gt;run&lt;span class="w"&gt; &lt;/span&gt;openai/gpt-4.1&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Tell me a joke&amp;quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;However, I was running into a few challenges with token limits, and usage limits on GitHub Models, so I wanted to switch to Azure AI Foundry instead - I thought it'd be a great opportunity to try out &lt;a href="https://github.com/simonw/llm"&gt;Simon's hugely popular LLM CLI tool&lt;/a&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;llm&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Tell me a joke&amp;quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h3&gt;Introduction - what is LLM?&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://github.com/simonw/llm"&gt;LLM&lt;/a&gt; is an open source CLI tool and Python library for interacting with OpenAI, Anthropic’s Claude, Google’s Gemini, Meta’s Llama and dozens of other Large Language Models, both via remote APIs and with models that can be installed and run on your own machine. It's extensible via a vast array of plugins, but supports OpenAI's models out of the box, and &lt;a href="https://llm.datasette.io/en/stable/other-models.html"&gt;"OpenAI-API-compatible" models via a configuration file&lt;/a&gt;. Because Azure OpenAI models are "OpenAI-API-compatible, I opted for the simple approach using the basic &lt;code&gt;llm&lt;/code&gt; tool with no plugins, and the configuration file.&lt;/p&gt;
&lt;h3&gt;1. Installation&lt;/h3&gt;
&lt;p&gt;Installation was a breeze - with a simple:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;pip&lt;span class="w"&gt; &lt;/span&gt;install&lt;span class="w"&gt; &lt;/span&gt;llm
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h3&gt;2. Storing an API key&lt;/h3&gt;
&lt;p&gt;Next, I needed to store my Azure AI Foundry API key, retrieved from the &lt;a href="https://ai.azure.com/foundryProject/overview"&gt;Azure AI Foundry project overview:&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img width="1025" height="425" alt="image" src="https://github.com/user-attachments/assets/60b03e48-598a-42fc-a447-9747559bae23" /&gt;&lt;/p&gt;
&lt;p&gt;By default, &lt;code&gt;llm keys set&lt;/code&gt; will store an OpenAI API key, so adding &lt;code&gt;azure&lt;/code&gt; on the end allowed me to differenciate it later&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;llm&lt;span class="w"&gt; &lt;/span&gt;keys&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;set&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;azure
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;When entering the key, the text isn't echoed back to the console, but you can check the keys.json file in the next step if you need to confirm.&lt;/p&gt;
&lt;h3&gt;3. Configuring the Azure AI Foundry Models&lt;/h3&gt;
&lt;p&gt;Before the next step, you'll want to check the path to your llm configuration files:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nb"&gt;PS &lt;/span&gt;&lt;span class="n"&gt;C&lt;/span&gt;&lt;span class="p"&gt;:\&lt;/span&gt;&lt;span class="n"&gt;Users&lt;/span&gt;&lt;span class="p"&gt;\&lt;/span&gt;&lt;span class="n"&gt;gugregor&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;llm&lt;/span&gt; &lt;span class="n"&gt;keys&lt;/span&gt; &lt;span class="n"&gt;path&lt;/span&gt;
&lt;span class="n"&gt;C&lt;/span&gt;&lt;span class="p"&gt;:\&lt;/span&gt;&lt;span class="n"&gt;Users&lt;/span&gt;&lt;span class="p"&gt;\&lt;/span&gt;&lt;span class="n"&gt;gugregor&lt;/span&gt;&lt;span class="p"&gt;\&lt;/span&gt;&lt;span class="n"&gt;AppData&lt;/span&gt;&lt;span class="p"&gt;\&lt;/span&gt;&lt;span class="n"&gt;Roaming&lt;/span&gt;&lt;span class="p"&gt;\&lt;/span&gt;&lt;span class="n"&gt;io&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;datasette&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;llm&lt;/span&gt;&lt;span class="p"&gt;\&lt;/span&gt;&lt;span class="n"&gt;keys&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;json&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;...so you can open the folder, and create a new (blank text) file called &lt;code&gt;extra-openai-models.yaml&lt;/code&gt;. This file will be used to define any models from Azure AI Foundry.&lt;/p&gt;
&lt;p&gt;&lt;img width="978" height="474" alt="image" src="https://github.com/user-attachments/assets/942e1fcc-d1ae-44c1-8707-b70cb64b2aac" /&gt;&lt;/p&gt;
&lt;p&gt;In this YAML file, you'll want to provide the details of your Azure AI Foundry model deployments:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="p p-Indicator"&gt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nt"&gt;model_id&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;aoai/gpt-5&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;model_name&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;gpt-5&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;api_base&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;https://&amp;lt;foundry resource&amp;gt;.openai.azure.com/openai/v1/&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;api_key_name&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;azure&lt;/span&gt;

&lt;span class="p p-Indicator"&gt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nt"&gt;model_id&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;aoai/gpt-5-mini&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;model_name&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;gpt-5-mini&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;api_base&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;https://&amp;lt;foundry resource&amp;gt;.openai.azure.com/openai/v1/&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;api_key_name&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;azure&lt;/span&gt;

&lt;span class="p p-Indicator"&gt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nt"&gt;model_id&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;aoai/gpt-5-nano&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;model_name&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;gpt-5-nano&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;api_base&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;https://&amp;lt;foundry resource&amp;gt;.openai.azure.com/openai/v1/&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;api_key_name&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;azure&lt;/span&gt;

&lt;span class="p p-Indicator"&gt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nt"&gt;model_id&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;aoai/gpt-4.1&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;model_name&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;gpt-4.1&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;api_base&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;https://&amp;lt;foundry resource&amp;gt;.openai.azure.com/openai/v1/&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;api_key_name&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;azure&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Some important Azure-specific considerations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;model_id&lt;/code&gt; is essentially the 'friendly name' for the model within the LLM tool. I chose a &lt;code&gt;aoai/&lt;/code&gt; prefix, so I could differenciate between Azure models, and OpenAI API models.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;model_name&lt;/code&gt; is the Azure deployment name - which &lt;em&gt;could&lt;/em&gt; be different from the model name (although it makes sense to keep it the same where possible).&lt;/li&gt;
&lt;li&gt;&lt;code&gt;api_base&lt;/code&gt; needs to include the &lt;code&gt;openai/v1/&lt;/code&gt; suffix, because the LLM tool isn't able to accept the &lt;code&gt;api_version&lt;/code&gt; from the legacy API. If you're not sure where to find the &lt;foundry resource&gt;, check in the &lt;a href="https://ai.azure.com/foundryProject/overview"&gt;Azure AI Foundry project overview:&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;api_key_name&lt;/code&gt; is the name of the key you stored in step 2 (I used &lt;code&gt;azure&lt;/code&gt;, but you can use whatever you like, as long as they match)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Don't forget to save the YAML file, once you've added all the above details.&lt;/p&gt;
&lt;h3&gt;4. Testing the Azure AI Foundry model&lt;/h3&gt;
&lt;p&gt;Now that you've configured the extra models, you should be able to run &lt;code&gt;llm&lt;/code&gt; using the following command:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;llm&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Tell me a joke&amp;quot;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;-m&lt;span class="w"&gt; &lt;/span&gt;aoai/gpt-5-mini
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;the &lt;code&gt;-m&lt;/code&gt; parameter specifies the model that you've defined in the YAML file from step 3.&lt;/p&gt;
&lt;h3&gt;5. Setting the default model (optional, recommended)&lt;/h3&gt;
&lt;p&gt;If you're going to use Azure AI Foundry models regularly, then you may want to change the default model over like this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;llm&lt;span class="w"&gt; &lt;/span&gt;models&lt;span class="w"&gt; &lt;/span&gt;default&lt;span class="w"&gt; &lt;/span&gt;aoai/gpt-5-mini
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;That way, you don't need to specify the model using the &lt;code&gt;-m&lt;/code&gt; parameter each time, so you get an easy to remember, concise command:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;llm&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Tell me a joke&amp;quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h3&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;LLM is a super handy utility which can easily be configured to use Azure AI Foundry models with minimal effort. I can see myself using it for a range of simple command-line tasks, and potentially even for more advanced scripting. It doesn't have the power or advanced features of something like Semantic Kernel, but that level of functionality isn't always required. Try it out today!&lt;/p&gt;
&lt;p&gt;LLM - GitHub.com&lt;br&gt;
&lt;a href="https://github.com/simonw/llm"&gt;https://github.com/simonw/llm&lt;/a&gt;&lt;/p&gt;</content><category term="Blog"/><category term="azure"/></entry><entry><title>AI-powered exam dashboard</title><link href="https://pedanticjournal.com/exam-timeline/" rel="alternate"/><published>2025-07-31T00:00:00+01:00</published><updated>2025-07-31T00:00:00+01:00</updated><author><name>Guy Gregory</name></author><id>tag:pedanticjournal.com,2025-07-31:/exam-timeline/</id><summary type="html">&lt;p&gt;How to build yourself a free, AI-powered Microsoft exam dashboard, using Python, GitHub Actions, GitHub Models, and Azure Static Web Apps&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;a href="https://exams.guygregory.com"&gt;&lt;img width="2217" height="1503" alt="image" src="https://github.com/user-attachments/assets/3aef88b7-aa2e-4d25-9d3b-b0d76bdd7766" /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;After over 20 years spent earning Microsoft certifications, and with my recent success on the &lt;a href="https://learn.microsoft.com/credentials/certifications/github-foundations/"&gt;GH-900&lt;/a&gt; exam, I started reflecting on just how far I’ve come. That’s when the idea for &lt;a href="https://github.com/guygregory/exam-timeline"&gt;&lt;code&gt;exam-timeline&lt;/code&gt;&lt;/a&gt; was born. I wanted a fun, interactive way to visualise my certification journey and to try building a simple AI app that used LLMs from &lt;a href="https://gh.io/models"&gt;GitHub Models&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The initial prototype came together in under half an hour of vibe-coding, thanks to &lt;a href="https://github.com/copilot"&gt;GitHub Copilot&lt;/a&gt; for the coding assist and using the Free tier of &lt;a href="https://learn.microsoft.com/azure/static-web-apps/overview"&gt;Azure Static Web Apps&lt;/a&gt; for the super-quick deployment and tight GitHub integration. From there, I spent a few more hours automating the extraction of exam data, and wiring up &lt;a href="https://docs.github.com/en/actions"&gt;GitHub Actions&lt;/a&gt; - mainly delegating the hard work to the &lt;a href="https://docs.github.com/en/copilot/how-tos/agents/copilot-coding-agent"&gt;GitHub Coding Agent&lt;/a&gt;. A few days later, I added the "AI recommendation" feature, using a few lines of Python. The end result is a project that’s both personal and practical, with a workflow that anyone can replicate.&lt;/p&gt;
&lt;h3&gt;How it works - AI-powered recommendation via GitHub Models&lt;/h3&gt;
&lt;p&gt;Let's start with the the feature I'm personally most excited about - The AI recommendation to suggest the 'next logical exam'.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://exams.guygregory.com"&gt;&lt;img width="419" height="168" alt="image" src="https://github.com/user-attachments/assets/525fc150-ab85-46f9-9dd9-050703afd9cd" /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;After the transcript is downloaded from Microsoft Learn, the workflow calls &lt;a href="https://github.com/guygregory/exam-timeline/blob/main/ai_exam_recommender.py"&gt;a Python script&lt;/a&gt;, and this inserts the learner's transcript into the user prompt of the LLM.&lt;/li&gt;
&lt;li&gt;The system prompt guides the LLM to make a recommendation for a next logical exam, and to avoid choosing an exam that the learner has already completed.&lt;/li&gt;
&lt;li&gt;The script uses OpenAI's gpt-4o model, which is hosted by GitHub Models (on Azure AI)&lt;/li&gt;
&lt;li&gt;Because this script is called directly from GitHub Actions, &lt;a href="https://docs.github.com/en/github-models/use-github-models/integrating-ai-models-into-your-development-workflow#using-ai-models-with-github-actions"&gt;auth works seamlessly&lt;/a&gt;. I only had to add &lt;code&gt;models: read&lt;/code&gt; into the &lt;code&gt;permissions&lt;/code&gt; section of the workflow for it to work.&lt;/li&gt;
&lt;li&gt;In order to return a consistent response, &lt;a href="https://learn.microsoft.com/azure/ai-foundry/openai/how-to/structured-outputs?tabs=python-secure%2Cdotnet-entra-id&amp;amp;pivots=programming-language-python"&gt;Structured Outputs&lt;/a&gt; are used with the &lt;a href="https://learn.microsoft.com/azure/ai-foundry/openai/how-to/structured-outputs?tabs=python-secure%2Cdotnet-entra-id&amp;amp;pivots=programming-language-python#supported-schemas-and-limitations"&gt;enum type&lt;/a&gt; to only return an answer from the &lt;a href="https://github.com/guygregory/exam-timeline/blob/main/priority_ARB_exams.csv"&gt;list of prioritised exams&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;The output of this script is a JSON object which includes the recommendation: &lt;code&gt;{"exam_code":"AZ-305"}&lt;/code&gt;, which is then inserted into the button in &lt;a href="https://github.com/guygregory/exam-timeline/blob/main/index.html"&gt;&lt;code&gt;index.html&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;This automation uses the &lt;a href="https://docs.github.com/en/github-models/prototyping-with-ai-models#rate-limits"&gt;GitHub Models quota, included in the GitHub Copilot plans (including free)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;How it works - Data extraction from Microsoft Learn&lt;/h3&gt;
&lt;p&gt;To extract the exam information, the project uses a &lt;a href="https://github.com/guygregory/exam-timeline/blob/main/passed_exams.py"&gt;Python script&lt;/a&gt; to download the Microsoft certification transcript, based on the &lt;a href="https://learn.microsoft.com/users/me/transcript"&gt;Transcript sharing code&lt;/a&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Disclaimer:&lt;/strong&gt; The use of the Microsoft Learn API in this way is &lt;strong&gt;not officially supported or documented&lt;/strong&gt;, and while suitable for a simple hobby project, is &lt;strong&gt;not appropriate for a production application&lt;/strong&gt;. Future API availability is not guaranteed. For commercial integrations, please contact your Microsoft representative.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;You can run this script independently if you just want a quick export for your own records or to feed into another tool. But if you’re feeling ambitious, you can &lt;a href="https://github.com/guygregory/exam-timeline"&gt;clone the entire repo&lt;/a&gt;, customise it, and deploy your own version in minutes. The daily automation fetches your latest transcript, stores it in the repo as a .csv file, which feeds a simple Plotly-powered (JS) dashboard. Here's how it gets the transcript from Microsoft Learn:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;API_ENDPOINT_TEMPLATE&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;https://learn.microsoft.com/api/profiles/transcript/share/&lt;/span&gt;&lt;span class="si"&gt;{share_id}&lt;/span&gt;&lt;span class="s2"&gt;?locale=&lt;/span&gt;&lt;span class="si"&gt;{locale}&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;

&lt;span class="n"&gt;url&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;API_ENDPOINT_TEMPLATE&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;share_id&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;share_id&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;locale&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;locale&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;headers&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
     &lt;span class="c1"&gt;# Provide a User‑Agent to avoid potential filtering of generic requests&lt;/span&gt;
     &lt;span class="s2"&gt;&amp;quot;User-Agent&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Mozilla/5.0 (compatible; MSFTTranscriptFetcher/1.0)&amp;quot;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="n"&gt;response&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;requests&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;url&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;headers&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;headers&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;response&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;raise_for_status&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;response&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;json&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;alternatively, using cURL&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;curl&lt;span class="w"&gt; &lt;/span&gt;-f&lt;span class="w"&gt; &lt;/span&gt;-H&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;User-Agent: Mozilla/5.0 (compatible; MSFTTranscriptFetcher/1.0)&amp;quot;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;https://learn.microsoft.com/api/profiles/transcript/share/&lt;/span&gt;&lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;share_id&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;?locale=&lt;/span&gt;&lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;locale&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;If you want to try this yourself, just remember: your Microsoft transcript may contain sensitive personal information, like your name and email address. Thankfully, &lt;a href="https://learn.microsoft.com/users/me/transcript"&gt;Microsoft Learn&lt;/a&gt; lets you adjust this before sharing anything publicly.&lt;/p&gt;
&lt;p&gt;&lt;img width="756" height="452" alt="image" src="https://github.com/user-attachments/assets/ccaca094-8d3f-41e5-9095-1d145bb80559" /&gt;&lt;/p&gt;
&lt;p&gt;Spending time on this project really helped me deepen my knowledge around GitHub Actions, and especially how they can be integrated with GitHub Models for &lt;a href="https://githubnext.com/projects/continuous-ai/"&gt;"Continuous AI"&lt;/a&gt;. It also made me consider how in future, we might AI-enable data like this using broadly adopted standards like MCP. Big thanks to a couple of colleagues for their input and advice on this project, namely fellow PSA, Bojan Vrhovnik and Allison Waldmann from the Microsoft Learn platform team.&lt;/p&gt;
&lt;p&gt;Finally, if you're looking to experiment with Python, GitHub Actions, GitHub Models, and Azure Static Websites, I hope my little &lt;code&gt;exam-timeline&lt;/code&gt; project inspires you to take pride in your learning journey. Contributions welcome!&lt;/p&gt;</content><category term="Blog"/><category term="github"/><category term="mslearn"/><category term="azure"/></entry></feed>