<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Pedantic Journal - Blog</title><link href="https://pedanticjournal.com/" rel="alternate"/><link href="https://pedanticjournal.com/feeds/blog.atom.xml" rel="self"/><id>https://pedanticjournal.com/</id><updated>2025-07-23T00:00:00+01:00</updated><subtitle>Thoughts on AI and other subjects.</subtitle><entry><title>Visualising Microsoft exams</title><link href="https://pedanticjournal.com/exam-timeline/" rel="alternate"/><published>2025-07-23T00:00:00+01:00</published><updated>2025-07-23T00:00:00+01:00</updated><author><name>Guy Gregory</name></author><id>tag:pedanticjournal.com,2025-07-23:/exam-timeline/</id><summary type="html">&lt;p&gt;How to build yourself a free automated Microsoft exam dashboard, using Python, GitHub Actions, and Azure Static Web Apps&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;a href="https://exams.guygregory.com" target="_blank" rel="noopener noreferrer"&gt;&lt;img width="2217" height="1503" alt="image" src="https://github.com/user-attachments/assets/708b3d1c-3e3d-48c3-aefb-74d983a85d00" /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;After over 20 years spent earning Microsoft certifications, and with my recent success on the &lt;a href="https://learn.microsoft.com/credentials/certifications/github-foundations/"&gt;GH-900&lt;/a&gt; exam, I started reflecting on just how far I’ve come. That’s when the idea for &lt;a href="https://github.com/guygregory/exam-timeline"&gt;&lt;code&gt;exam-timeline&lt;/code&gt;&lt;/a&gt; was born. I wanted a fun, interactive way to visualise my certification journey and maybe inspire others who are on a similar path. There’s something satisfying about seeing all those milestones lined up, and I thought: why not make it easy for anyone to do the same?&lt;/p&gt;
&lt;p&gt;The initial prototype came together in under half an hour of vibe-coding, thanks to &lt;a href="https://github.com/copilot"&gt;GitHub Copilot&lt;/a&gt; for the coding assist and using the Free tier of &lt;a href="https://learn.microsoft.com/azure/static-web-apps/overview"&gt;Azure Static Web Apps&lt;/a&gt; for the super-quick deployment and tight GitHub integration. From there, I spent a few more hours automating the extraction of exam data, and wiring up &lt;a href="https://docs.github.com/en/actions"&gt;GitHub Actions&lt;/a&gt; - mainly delegating the hard work to the &lt;a href="https://docs.github.com/en/copilot/how-tos/agents/copilot-coding-agent"&gt;GitHub Coding Agent&lt;/a&gt;. The end result is a project that’s both personal and practical, with a workflow that anyone can replicate.&lt;/p&gt;
&lt;p&gt;Here’s how it all works: at its core, the project uses a &lt;a href="https://github.com/guygregory/exam-timeline/blob/main/passed_exams.py"&gt;Python script&lt;/a&gt; to download your Microsoft certification transcript, based on your &lt;a href="https://learn.microsoft.com/users/me/transcript"&gt;Transcript sharing code&lt;/a&gt;. You can run this script independently if you just want a quick export for your own records or to feed into another tool. But if you’re feeling ambitious, you can &lt;a href="https://github.com/guygregory/exam-timeline"&gt;clone the entire repo&lt;/a&gt;, customise it, and deploy your own version in minutes. The daily automation fetches your latest transcript, stores it in the repo as a .csv file, which feeds a simple Plotly-powered (JS) dashboard. Here's how it gets the transcript from Microsoft Learn:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;API_ENDPOINT_TEMPLATE&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;https://learn.microsoft.com/api/profiles/transcript/share/&lt;/span&gt;&lt;span class="si"&gt;{share_id}&lt;/span&gt;&lt;span class="s2"&gt;?locale=&lt;/span&gt;&lt;span class="si"&gt;{locale}&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;

&lt;span class="n"&gt;url&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;API_ENDPOINT_TEMPLATE&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;share_id&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;share_id&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;locale&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;locale&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;headers&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
     &lt;span class="c1"&gt;# Provide a User‑Agent to avoid potential filtering of generic requests&lt;/span&gt;
     &lt;span class="s2"&gt;&amp;quot;User-Agent&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Mozilla/5.0 (compatible; MSFTTranscriptFetcher/1.0)&amp;quot;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="n"&gt;response&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;requests&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;url&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;headers&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;headers&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;response&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;raise_for_status&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;response&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;json&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;If you want to try this yourself, just remember: your Microsoft transcript may contain sensitive personal information, like your name and email address. Thankfully, &lt;a href="https://learn.microsoft.com/users/me/transcript"&gt;Microsoft Learn&lt;/a&gt; lets you adjust this before sharing anything publicly.&lt;/p&gt;
&lt;p&gt;&lt;img width="756" height="452" alt="image" src="https://github.com/user-attachments/assets/ccaca094-8d3f-41e5-9095-1d145bb80559" /&gt;&lt;/p&gt;
&lt;p&gt;Whether you’re looking to track your own certification milestones, show off your progress, or just experiment with Python, GitHub Actions, and Azure Static Websites, I hope my little &lt;code&gt;exam-timeline&lt;/code&gt; project inspires you to take pride in your learning journey. Contributions welcome - let’s celebrate our wins together!&lt;/p&gt;</content><category term="Blog"/><category term="github"/><category term="mslearn"/><category term="azure"/></entry><entry><title>Responses API on Azure OpenAI</title><link href="https://pedanticjournal.com/responses-api/" rel="alternate"/><published>2025-03-11T00:00:00+00:00</published><updated>2025-03-11T00:00:00+00:00</updated><author><name>Guy Gregory</name></author><id>tag:pedanticjournal.com,2025-03-11:/responses-api/</id><summary type="html">&lt;p&gt;Responses API combines the simplicity of Chat Completions with the tool use and state management of the Assistants API.&lt;/p&gt;</summary><content type="html">&lt;p&gt;The Responses API is a new stateful API from OpenAI. It brings together the best capabilities from the chat completions and assistants API in one unified experience. The Responses API also adds support for the new computer-use-preview model which powers the Computer use capability.&lt;/p&gt;
&lt;p&gt;I've shared some basic code samples on my &lt;a href="https://aka.ms/ResponsesAPI"&gt;GitHub repo&lt;/a&gt; for those who want to get started using Responses API on Azure OpenAI:&lt;/p&gt;
&lt;p&gt;Responses API GitHub repo:&lt;br&gt;
&lt;a href="https://aka.ms/ResponsesAPI"&gt;https://aka.ms/ResponsesAPI&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Microsoft Learn Documentation:&lt;br&gt;
&lt;a href="https://aka.ms/ResponsesAPI/Docs"&gt;https://aka.ms/ResponsesAPI/Docs&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Announcement blog post:&lt;br&gt;
&lt;a href="https://azure.microsoft.com/blog/announcing-the-responses-api-and-computer-using-agent-in-azure-ai-foundry/"&gt;https://azure.microsoft.com/blog/announcing-the-responses-api-and-computer-using-agent-in-azure-ai-foundry/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Request access to the computer-use-preview model:&lt;br&gt;
&lt;a href="https://aka.ms/oai/cuaaccess"&gt;https://aka.ms/oai/cuaaccess&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Here's a basic example from the &lt;a href="https://aka.ms/ResponsesAPI"&gt;repo&lt;/a&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;os&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;openai&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;AzureOpenAI&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;dotenv&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;load_dotenv&lt;/span&gt;

&lt;span class="n"&gt;load_dotenv&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="n"&gt;client&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;AzureOpenAI&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;api_key&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;os&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;environ&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;AZURE_OPENAI_API_KEY&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;  
    &lt;span class="n"&gt;api_version&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;os&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;environ&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;AZURE_OPENAI_API_VERSION&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="n"&gt;azure_endpoint&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;os&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;environ&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;AZURE_OPENAI_API_ENDPOINT&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;response&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;client&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;responses&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;create&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;os&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;environ&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;AZURE_OPENAI_API_MODEL&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="nb"&gt;input&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Tell me a joke.&amp;quot;&lt;/span&gt;

&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;response&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;output_text&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</content><category term="Blog"/><category term="azureai"/><category term="openai"/></entry><entry><title>Function calling with voice</title><link href="https://pedanticjournal.com/function-calling-voice/" rel="alternate"/><published>2024-10-18T00:00:00+01:00</published><updated>2024-10-18T00:00:00+01:00</updated><author><name>Guy Gregory</name></author><id>tag:pedanticjournal.com,2024-10-18:/function-calling-voice/</id><summary type="html">&lt;p&gt;Using Realtime API with gpt-4o to access tools&lt;/p&gt;</summary><content type="html">&lt;p&gt;Last week, after presenting the new Realtime API for speech and audio in Azure OpenAI Service, a Microsoft partner asked me whether it's possible to integrate external tools and services.&lt;/p&gt;
&lt;p&gt;The answer? Absolutely! In fact, our public &lt;a href="https://github.com/Azure-Samples/aisearch-openai-rag-audio"&gt;VoiceRAG demo&lt;/a&gt; itself uses function calling for Azure AI Search integration, and it's relatively easy to describe your own custom functions (using natural language).&lt;/p&gt;
&lt;p&gt;For my own learning, I thought I'd try writing a couple of custom functions myself this week, and I wanted to share the result. The scenario in my new demo is a customer calling a garage, "Fabrikam Motors", to book a car service:&lt;/p&gt;
&lt;iframe width="560" height="315" src="https://www.youtube.com/embed/0eZEvEilFBI?si=K8r0JyE4T8qJ4kUU" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen&gt;&lt;/iframe&gt;

&lt;p&gt;Here's the actual prompt I'm using:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;"You are an AI assistant working for Fabrikam Motors that helps people book their annual car service and MOT test in the UK. Today's date is " + today_date + ". When looking for available booking dates, you must always use the 'get_available_booking_days' tool. Never make up dates yourself. When looking up car information, you must always use the 'get_car_info' tool. Never make up car information yourself. Follow this script: 1. Greet the caller as soon as the call starts, 'Hi, welcome to Fabrikam Motors, how can I help you today?'2. If the caller would like an MOT or annual car service, please ask them for their vehicle registration number.3. Read back the vehicle registration number to them, and ask them to confirm the details are correct.4. If incorrect repeat steps 2 and 3 until the caller has confirmed the details are correct.5. Use the 'get_car_info' tool to look up the car's colour, manufacturer, and MOT expiry date using the confirmed vehicle registration number. Do not place the caller on hold.6. Wait a few seconds, and then verify the tool output with the caller, confirming the car's colour, manufacturer, and MOT expiry date.7. Use the 'get_available_booking_days' tool to look up available booking dates, and provide these options to the caller.8. Confirm the caller's preferred date, and tell them they'll receive a confirmation text message to their phone.9. Ask if there are any other issues with the car that need to be addressed during the service.10. Close the call, thanking the caller for the booking, and reminding them bring the car's locking wheel nut key and vehicle service logbook.11. Say goodbye."&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;During the call, the AI agent takes the UK registration plate (step 2), and via a function call, queries an public vehicle information API for the car's details (step 5). There's also a second function call which provides booking availability, which returns some mocked date options (step 7).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Here are my top ten learnings I wanted to share from trying this out:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;1. This demo seems daunting, but you can get started in minutes. Simply deploy an Azure OpenAI Service instance in East US 2 or Sweden Central, and use &lt;a href="https://ai.azure.com/"&gt;Azure AI Studio&lt;/a&gt; to immediately try the real-time audio service - no code needed. For starters, just use hard-coded values instead of function calls if you want to get a feel for the conversation flow.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Screenshot of Azure OpenAI Service Realtime API demo" src="https://media.licdn.com/dms/image/v2/D4E12AQFc83jQtGvKBQ/article-inline_image-shrink_1500_2232/article-inline_image-shrink_1500_2232/0/1729244709853?e=1753920000&amp;amp;v=beta&amp;amp;t=n6-HBCwUEd67IIYp0l2P9wlAplq1Xaf4MfT23dejGh4"&gt;&lt;/p&gt;
&lt;p&gt;2. Cloning the existing &lt;a href="https://github.com/Azure-Samples/aisearch-openai-rag-audio"&gt;VoiceRAG demo&lt;/a&gt; repo is a really great way to get a working demo up and running quickly. This sample can now run inside a &lt;a href="https://github.com/features/codespaces"&gt;GitHub Codespaces&lt;/a&gt; environment, giving you loads of flexibility and no requirement to install anything locally. It also helps you understand how tools are defined. GitHub Codespaces is free for individual use up to 60 hours a month.&lt;/p&gt;
&lt;p&gt;3. To get the original &lt;a href="https://github.com/Azure-Samples/aisearch-openai-rag-audio"&gt;VoiceRAG demo&lt;/a&gt; working with your own Azure OpenAI Service and Azure AI Search instance, you'll need to create a .env file in the /app/backend directory. This is also where I stored my external API key for the vehicle lookup:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;AZURE_OPENAI_ENDPOINT&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;https://xxxxxxxxx.openai.azure.com&amp;quot;&lt;/span&gt;
&lt;span class="n"&gt;AZURE_OPENAI_REALTIME_DEPLOYMENT&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;gpt-4o-realtime-preview&amp;quot;&lt;/span&gt;
&lt;span class="n"&gt;AZURE_OPENAI_API_KEY&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx&amp;quot;&lt;/span&gt;
&lt;span class="n"&gt;AZURE_SEARCH_ENDPOINT&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;https://xxxxxxxxxx.search.windows.net&amp;quot;&lt;/span&gt;
&lt;span class="n"&gt;AZURE_SEARCH_INDEX&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&amp;lt;indexname&amp;gt;&amp;quot;&lt;/span&gt;
&lt;span class="n"&gt;AZURE_SEARCH_API_KEY&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx&amp;quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;4. The Azure AI Search element was the most costly part of this demo for me, and I used the Basic tier (which is &lt;a href="https://azure.microsoft.com/en-us/pricing/details/search/?msockid=3e6f21cebcd26bca2235355ebda86a3f#pricing"&gt;approx. $74/£55&lt;/a&gt; at time of writing). If you're building a demo that just calls an API, and doesn't call Azure AI Search at all, then you don't &lt;em&gt;need &lt;/em&gt;to deploy it.&lt;/p&gt;
&lt;p&gt;5. Multiple tool definitions &lt;em&gt;are&lt;/em&gt; supported, so if you do want to keep the existing Azure AI Search tool definition from the demo to implement a new one, then it can quite happily exist side-by-side with the custom tools you add.&lt;/p&gt;
&lt;p&gt;6. If you want to create a function call to an API using voice, it can feel quite challenging at first. I broke down the task into smaller tasks to make it more digestible:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;First, get your API working with a simple API call code sample. No AI integration at all.&lt;/li&gt;
&lt;li&gt;Next, build a tool function for your API call, which works with the basic text input/output. This helps build confidence around using a tool to query an API with function calling. If you haven't done this before, there are some great code samples in &lt;a href="https://learn.microsoft.com/azure/ai-services/openai/how-to/function-calling"&gt;Microsoft Learn&lt;/a&gt;, and in &lt;a href="https://gh.io/models"&gt;GitHub Models&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Then using the Azure AI Search tool as an example, describe a function which provides a simple, mocked answer without any API call (in my case, the booking dates).&lt;/li&gt;
&lt;li&gt;Building on all of the previous knowledge, now write a function which includes the API call.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;7. I used both GitHub Copilot and o1-preview extensively to help me write and debug the code to define the tools faster. In my tool creation prompts, I would include the existing code for the existing tools (in ragtools.py), and my basic API code, and asked the model to create a new tool based on those inputs.&lt;/p&gt;
&lt;p&gt;8. Creating a prompt for the Realtime API is similar to doing it for text/image, but requires a bit of practice to get right. Especially when it comes to timing, and ensuring the AI doesn't go off-script.&lt;/p&gt;
&lt;p&gt;9. It's worth noting that gpt-4o-realtime-preview is free on Azure OpenAI until the end of October 2024, so now is a great time to experiment with it.&lt;/p&gt;
&lt;p&gt;10. Finally, I haven't been able to get my demo working flawlessly every time yet - I think due to some bugs in my function calls (I'm honestly not an expert developer). I probably need to focus on tightening up the output of the function call to ensure that the tool provides the AI assistant with a more predicable, strictly defined format. However, I'm delighted that I managed to build something that works this well in only a single day of coding.&lt;/p&gt;
&lt;p&gt;To wrap up, even though this new Realtime API service is in preview, I think the extensibility aspect opens up some very exciting scenarios, and allows for relatively simple integration with existing systems. If you're a Microsoft partner looking to use this new feature to solve a customer challenge, I'd love to hear from you - please feel to drop me a message on here. Thanks for reading!&lt;/p&gt;</content><category term="Blog"/><category term="azureai"/><category term="voice"/></entry><entry><title>GPT-4o mini available in Azure OpenAI</title><link href="https://pedanticjournal.com/gpt-4o-mini/" rel="alternate"/><published>2024-08-01T00:00:00+01:00</published><updated>2024-08-01T00:00:00+01:00</updated><author><name>Guy Gregory</name></author><id>tag:pedanticjournal.com,2024-08-01:/gpt-4o-mini/</id><summary type="html">&lt;p&gt;Towards intelligence too cheap to meter&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;a href="https://techcommunity.microsoft.com/t5/ai-azure-ai-services-blog/openai-s-gpt-4o-mini-now-available-in-api-with-vision/ba-p/4200640"&gt;GPT-4o mini can now be deployed in the East US Azure region&lt;/a&gt;, with more regions coming soon. Having been available in preview for the last week via the Early Access Playground, partners and customers can now access GPT-4o mini via the Azure OpenAI API.&lt;/p&gt;
&lt;p&gt;This new, smaller version of OpenAI's GPT-4o model offers speed, capability, vision, 128k context window, all at a fraction of the cost of the standard GPT-4o model - and even cheaper than GPT-3.5 Turbo.&lt;/p&gt;
&lt;p&gt;GPT-4o mini will initially be available in Standard Global, and Standard Regional &lt;a href="https://learn.microsoft.com/azure/ai-services/openai/how-to/deployment-types"&gt;deployment types&lt;/a&gt;, with Batch, and Provisioned coming soon. Standard Global pricing is a staggering $0.15 per 1M tokens for input, and $0.60 per 1M tokens for output.&lt;/p&gt;
&lt;p&gt;Based on my own experience using GPT-4o mini in the Azure OpenAI Playground, and also how strongly it's scoring on independent evaluations, I'm convinced that it's going to quickly become the most popular model in the Azure OpenAI Service.&lt;/p&gt;
&lt;p&gt;&lt;img alt="image" src="https://github.com/user-attachments/assets/e2626dd4-31f4-4e2a-b03c-f859e314c3ea"&gt;&lt;/p&gt;
&lt;p&gt;If you want to try GPT-4o mini yourself, try deploying the model in &lt;a href="https://oai.azure.com/"&gt;Azure OpenAI Studio&lt;/a&gt;, or use one of the many &lt;a href="https://github.com/Azure-Samples/"&gt;code samples on GitHub&lt;/a&gt;. If you've never used the Azure OpenAI Service before, you'll be pleased to know that last month, the signup process was streamlined, removing the need to submit a manual form for access.&lt;/p&gt;</content><category term="Blog"/><category term="azureai"/><category term="openai"/></entry></feed>